{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-Step Walkthrough for Asking User Support\n",
    "A general recipe and step-by-step walkthrough for asking user support using LLMs' output token probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Base and Children LLM Classes\n",
    "For extracting token log probabilities more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class LLM(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, model_name: str = None, **kwargs) -> None:\n",
    "        \"\"\"Setup the model\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_tokens: int = 512,\n",
    "        temperature: float = 0.0,\n",
    "        top_logprobs: int = 5,  # NOTE: how many token logprobs per token position to return\n",
    "    ) -> tuple[str, list[dict[str, float]]]:\n",
    "        \"\"\"Generate text from the model, and return the (text, logprobs) tuple\n",
    "    \n",
    "        Returns: (text: str, logprobs: list[dict[str, float]])\n",
    "        Format of logprobs (suppose top_logprobs=5):\n",
    "        [\n",
    "            {<token_0>: <logprob>, <token_1>: <logprob>, ..., <token_4>: <logprob>},  # logprobs of top-5 tokens for position 0\n",
    "            {<token_0>: <logprob>, <token_1>: <logprob>, ..., <token_4>: <logprob>},  # logprobs of top-5 tokens for position 1\n",
    "            ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "# For example, we can implement OpenAI model endpoints as follows:\n",
    "class OpenAILLM(LLM):\n",
    "    def __init__(self, model_name: str = \"gpt-4o-mini-2024-07-18\", api_key: str = None) -> None:\n",
    "        if not api_key:\n",
    "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_tokens: int = 512,\n",
    "        temperature: float = 0.0,\n",
    "        top_logprobs: int = 20,  # NOTE: how many token logprobs per token position to return\n",
    "        **kwargs\n",
    "    ) -> tuple[str, list[dict[str, float]]]:\n",
    "        res = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            logprobs=True,\n",
    "            top_logprobs=top_logprobs,\n",
    "            **kwargs\n",
    "        )\n",
    "        text = res.choices[0].message.content\n",
    "        logprobs = [  # NOTE: each token_position has <top_logprobs> token choices\n",
    "            {choice.token: choice.logprob for choice in token_position.top_logprobs} for token_position in res.choices[0].logprobs.content\n",
    "        ]\n",
    "        return text, logprobs\n",
    "\n",
    "# Usage example:\n",
    "model_name = \"gpt-4o-mini-2024-07-18\"\n",
    "api_key = Path(\"../apikeys/openai_bench.txt\").read_text().strip()\n",
    "llm = OpenAILLM(model_name, api_key)\n",
    "\n",
    "text, logprobs = llm(prompt=\"Just say two tokens 'hello!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello!\n",
      "There are 2 tokens in the text:\n",
      "Top-20 token logprobs of the first token position: {'hello': -0.0019295862, 'Hello': -6.2519298, ' hello': -13.876929, 'Sure': -16.75193, 'hell': -18.00193, 'Certainly': -19.75193, 'hi': -19.87693, \"I'm\": -19.87693, '\"': -20.00193, \"'\": -20.75193, '`': -21.00193, 'I': -21.12693, '你好': -21.75193, 'The': -22.12693, 'Here': -22.12693, '**': -22.25193, 'Sorry': -22.25193, 'sure': -22.37693, '``': -22.87693, ' Hello': -22.87693}\n",
      "Top-20 token logprobs of the second token position: {'!': -1.2664457e-06, '!\\n': -13.750001, '!\\n\\n': -15.625001, '!\"': -19.000002, '!!': -19.375002, '！': -20.000002, ' !': -20.250002, '!</': -20.375002, \"!'\": -20.625002, '!【': -20.750002, '!*': -21.000002, '!)': -22.500002, '![': -22.500002, '!,': -22.750002, '!\\\\': -22.750002, '!\\n\\n\\n': -22.750002, '%!': -22.875002, '!important': -22.875002, '<|end|>': -22.875002, '!!!': -23.000002}\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(f\"There are {len(logprobs)} tokens in the text:\")\n",
    "print(f\"Top-20 token logprobs of the first token position: {logprobs[0]}\")\n",
    "print(f\"Top-20 token logprobs of the second token position: {logprobs[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Prompt for Asking User Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\\n",
    "You are currently doing the question-answering (QA) task. Based on the information provided, you have to determine whether additional information is required for you to answer the question correctly.\n",
    "\n",
    "Information provided (enclosed by triple backticks):\n",
    "```\n",
    "Question: {question}\n",
    "Information: {information}\n",
    "```\n",
    "\n",
    "Answer a single word Yes if you need additional information to answer the question, and No otherwise.\n",
    "Do you need additional information? Answer (Yes / No):\"\"\"\n",
    "\n",
    "question = \"Which company does Cheng-Kuang Wu work at?\"\n",
    "\n",
    "prompt_1 = prompt_template.format(\n",
    "    question=question,\n",
    "    information=\"\"  # no information provided\n",
    ")\n",
    "\n",
    "prompt_2 = prompt_template.format(\n",
    "    question=question,\n",
    "    information=\"Cheng-Kuang Wu is a research scientist at Appier, Bppier, Cppier, Dppier, or Eppier.\"  # Vague information provided\n",
    ")\n",
    "\n",
    "prompt_3 = prompt_template.format(\n",
    "    question=question,\n",
    "    information=\"Cheng-Kuang Wu is a research scientist at Appier or Bppier.\"  # information provided\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Get \"Need Hint\" Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_softmax_yes(logprob: dict[str, float]) -> float:\n",
    "    \"\"\"Get the softmaxed probability of the token 'Yes' among 'Yes' and 'No'\"\"\"\n",
    "    TOKEN_LIST = [\"Yes\", \"No\"]\n",
    "    probs = np.array([logprob[token] for token in TOKEN_LIST])\n",
    "    softmax_probs = np.exp(probs) / np.sum(np.exp(probs))\n",
    "    return softmax_probs[0]\n",
    "\n",
    "res_1, logprobs_1 = llm(prompt=prompt_1)\n",
    "res_2, logprobs_2 = llm(prompt=prompt_2)\n",
    "res_3, logprobs_3 = llm(prompt=prompt_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you need hint for (prompt_1, prompt_2, prompt_3)?: ('Yes', 'Yes', 'No')\n",
      "Need-user-support probability (softmax of 'Yes') for prompt_1: 100.00%\n",
      "Need-user-support probability (softmax of 'Yes') for prompt_2: 98.90%\n",
      "Need-user-support probability (softmax of 'Yes') for prompt_3: 22.27%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Do you need hint for (prompt_1, prompt_2, prompt_3)?: {res_1, res_2, res_3}\")\n",
    "print(f\"Need-user-support probability (softmax of 'Yes') for prompt_1: {get_softmax_yes(logprobs_1[0]) * 100:.2f}%\")\n",
    "print(f\"Need-user-support probability (softmax of 'Yes') for prompt_2: {get_softmax_yes(logprobs_2[0]) * 100:.2f}%\")\n",
    "print(f\"Need-user-support probability (softmax of 'Yes') for prompt_3: {get_softmax_yes(logprobs_3[0]) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
